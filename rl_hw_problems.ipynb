{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl_hw_problems.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "O0fFgFRGksH6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "\n",
        "To run this you need several packages. First of all, you need anaconda, which you most likely already have if you're viewing this through jupyter. If not then check the readme on the class page.\n",
        "\n",
        "System requirements: This should work on all operating systems (Linux, Mac, and Windows). However, several of the environments in the OpenAI-gym require additional simulators which don't aren't easy to get on Windows. In any case, it is strongly recommended that you use Linux, although you should be ok with Mac. (HINT: if you're on Windows check out the Windows Subsystem for Linux (WSL), although it'll make visualizing your policies a little tricky).\n",
        "\n",
        "Then install the following packages (using conda or pip):\n",
        "\n",
        "- pytorch --> `conda install pytorch -c pytorch`\n",
        "- gym --> `pip install gym`\n",
        "- gym (the cool environments, doesnt work on Windows) --> `pip install gym[all]`\n",
        "(When install gym[all] don't worry if the mujoco installation doesn't work. That's a more advanced 3D physics simulator that has to be set up separately (see website). Anyway, we don't need it necessarily)."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nomKMuz0lrWL",
        "outputId": "ed9d2896-6d43-48ce-dbd3-73108406e860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        }
      },
      "cell_type": "code",
      "source": [
        "# If you're using colab, this will install the necessary packages!\n",
        "!pip install torch\n",
        "!pip install gym\n",
        "!pip install box2d-py\n",
        "!wget https://pjreddie.com/media/files/rlhw_util.py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 26kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x58576000 @  0x7f68f4bf42a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.1\n",
            "Collecting gym\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/22/4ff09745ade385ffe707fb5f053548f0f6a6e7d5e98a2b9d6c07f5b931a7/gym-0.10.9.tar.gz (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 14.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Collecting pyglet>=1.2.0 (from gym)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 21.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.10.15)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Running setup.py bdist_wheel for gym ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/6c/3a/0e/b86dee98876bb56cdb482cc1f72201035e46d1baf69d10d028\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, gym\n",
            "Successfully installed gym-0.10.9 pyglet-1.3.2\n",
            "Collecting box2d-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K    100% |████████████████████████████████| 450kB 9.9MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "--2018-11-30 04:36:02--  https://pjreddie.com/media/files/rlhw_util.py\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.3.39\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.3.39|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4071 (4.0K) [application/octet-stream]\n",
            "Saving to: ‘rlhw_util.py’\n",
            "\n",
            "rlhw_util.py        100%[===================>]   3.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2018-11-30 04:36:02 (317 MB/s) - ‘rlhw_util.py’ saved [4071/4071]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NzmPUAD1ksH7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys, os, time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.multiprocessing as mp\n",
        "from torch import distributions\n",
        "from torch.distributions import Categorical\n",
        "from itertools import islice\n",
        "\n",
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WXry0cZLksH_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "Welcome to the RL playground. Your task is to implement the REINFORCE and A3C algorithm to solve various OpenAI-gym environments. If you are not familiar with OpenAI-gym, stop reading and visit https://gym.openai.com/envs/ to see all the tasks you can try to solve.\n",
        "\n",
        "In this homework, we will only look at tasks with a discrete (and small) action space. That being said, both algorithms can be modified slightly to work on tasks with continuous action spaces. For full credit you must fill in the code below so you achieve an average total reward per episode on the cartpole task (CartPole-v1) of at least 499 (for an episode length of 500) for both REINFORCE and A3C. Then you must apply your code to any one other environment in OpenAI-gym, and plot and compare the learning curves (average total reward per episode vs number of episodes trained on) between REINFORCE and A3C (where at least one of the algorithms shows significant improvement from initialization).\n",
        "\n",
        "Below there's an overview of what every iteration will look like, regardless of whether you want to train or evaluate your agent."
      ]
    },
    {
      "metadata": {
        "id": "IBY204WCksIA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from rlhw_util import * # <-- look whats inside here - it could save you a lot of work!\n",
        "\n",
        "def run_iteration(mode, N, agent, gen, horizon=None, render=False):\n",
        "    train = mode == 'train'\n",
        "    if train:\n",
        "        agent.train()\n",
        "    else:\n",
        "        agent.eval()\n",
        "\n",
        "    states, actions, rewards = zip(*[gen(horizon=horizon, render=render) for _ in range(N)])\n",
        "\n",
        "    loss = None\n",
        "    if train:\n",
        "        loss = agent.learn(states, actions, rewards)\n",
        "\n",
        "    reward = sum([r.sum() for r in rewards]) / N\n",
        "\n",
        "    return reward, loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FbOI_tXDksIC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Actor\n",
        "\n",
        "We need to learn a policy which, given some state, outputs a distribution over all possible actions. As this is deep RL, we'll use a deep neural network to turn the observed state into the requisite action distribution. From this action distribution we can choose what action to take using `get_action`. Pytorch, brilliant as it is, makes our task incredibly easy, as we can use the `torch.distributions.Categorical` class for sampling.\n",
        "\n",
        "You can experiment with all sorts of network architectures, but remember this is RL, not image classification on ImageNet, so you probably won't need a very deep network (HINT: look below at the state and action dimensionality to get a feel for the task)."
      ]
    },
    {
      "metadata": {
        "id": "QkW0tUvZksID",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Actor, self).__init__()\n",
        "        \n",
        "        # TODO: Fill in the code to define you policy\n",
        "        self.fc1 = nn.Linear(state_dim, (state_dim + action_dim) // 2)\n",
        "        self.fc2 = nn.Linear((state_dim + action_dim) // 2, action_dim)\n",
        "        self.fc9 = nn.Linear(state_dim, action_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        \n",
        "        #raise NotImplementedError\n",
        "        \n",
        "    def forward(self, state):\n",
        "        \n",
        "        # TODO: Fill in the code to run a forward pass of your policy to get a distribution over actions (HINT: probabilities sum to 1)\n",
        "        x = state\n",
        "        x = self.fc9(x)\n",
        "        return self.softmax(x)\n",
        "        #raise NotImplementedError\n",
        "\n",
        "    def get_policy(self, state):\n",
        "        return Categorical(self(state))\n",
        "\n",
        "    def get_action(self, state, greedy=None):\n",
        "        if greedy is None:\n",
        "            greedy = not self.training\n",
        "\n",
        "        policy = self.get_policy(state)\n",
        "        return MLE(policy) if greedy else policy.sample()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5NYbREzFksIF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The REINFORCE Agent\n",
        "\n",
        "The Actor defines our policy, but we also have to define how and when we'll be updating our policy, which brings us to the agent. The agent will house the policy (an `Actor`), and can then be used to generate rollouts (using `forward()`) or update the policy given a list of rollouts (using `learn()`).\n",
        "\n",
        "The REINFORCE algorithm naively uses the returns directly to weight the gradients, however this makes the variance in the policy gradient estimation very large. As a result, we will use a baseline which is a linear model which takes in a state and outputs the return (sounds like a value function, right?). Except we're not going to train our baseline using gradient descent, instead we'll just solve the linear system analytically in every iteration, and use the solution in the next iteration. Don't worry about training/updating the baseline, but you do have to use it in the right way. (Optional experiment: try removing the baseline and see how performance changes)"
      ]
    },
    {
      "metadata": {
        "id": "nQ7mlwvLksIG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class REINFORCE(nn.Module):\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, discount=0.97, lr=1e-3, weight_decay=1e-4):\n",
        "        super(REINFORCE, self).__init__()\n",
        "        self.actor = Actor(state_dim, action_dim)\n",
        "        \n",
        "        self.baseline = nn.Linear(state_dim, 1)\n",
        "        \n",
        "        self.optimizer = optim.SGD(self.actor.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "        self.discount = discount\n",
        "        \n",
        "    def forward(self, state):\n",
        "        return self.actor.get_action(state)\n",
        "    \n",
        "    def learn(self, states, actions, rewards):\n",
        "        '''\n",
        "        Takes in three arguments each of which is a list with equal length. Each element in the list is a \n",
        "        pytorch tensor with 1 row for every step in the episode, and the columns are state_dim, action_dim, \n",
        "        and 1, respectively.\n",
        "        '''\n",
        "        \n",
        "        # TODO: implement the REINFORCE algorithm (HINT: check the slides/papers)\n",
        "                \n",
        "        returns = [compute_returns(rs, discount=self.discount) for rs in rewards]\n",
        "        \n",
        "        states, actions, returns = torch.cat(states), torch.cat(actions), torch.cat(returns)\n",
        "\n",
        "        error = F.mse_loss(self.baseline(states).squeeze(), returns).detach()\n",
        "        solve(states, returns, out=self.baseline)\n",
        "        \n",
        "        Q_sa = returns\n",
        "        \n",
        "        V_st = self.baseline(states).squeeze()\n",
        "        \n",
        "        log = torch.log\n",
        "        \n",
        "        policy = self.actor.forward(states)\n",
        "        \n",
        "        # does this work?\n",
        "        #pi_theta = torch.gather(policy, 1, actions)\n",
        "        #pi_theta = torch.index_select()\n",
        "        pi_theta = torch.squeeze(policy.gather(1, actions.view(-1, 1)))\n",
        "        #print(pi_theta.shape)\n",
        "        #pi_theta = torch.tensor([policy[i][actions[i]] for i in range(policy.shape[0])]) \n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        loss = -((Q_sa - V_st) * log(pi_theta))\n",
        "                \n",
        "        loss.sum().backward()\n",
        "        \n",
        "        self.optimizer.step()\n",
        "\n",
        "        return error.item() # Returns a rough estimate of the error in the baseline (dont worry about this too much)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "47wcKUumA4AP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# m = torch.randn(4,2)\n",
        "# print(m)\n",
        "# ids = torch.Tensor([1,1,0,0]).long()\n",
        "# print(torch.squeeze(m.gather(1, ids.view(-1,1))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ogkj6xjeksII",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Critic\n",
        "\n",
        "Now we can introduce a critic, which is essentially a value function to estimate the expected discounted reward of a state."
      ]
    },
    {
      "metadata": {
        "id": "uGYwOY6AksIJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        \n",
        "        # TODO: define your value function network\n",
        "        \n",
        "        # just try a  fc nn that goes down to 1, I have not clue what else to do\n",
        "        self.fc1 = nn.Linear(state_dim, 1)\n",
        "        \n",
        "        #raise NotImplementedError\n",
        "\n",
        "    def forward(self, state):\n",
        "        \n",
        "        # TODO: apply your value function network to get a value given this batch of states\n",
        "        return self.fc1(state)\n",
        "        #raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5rfkylePksIL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The A3C Agent\n",
        "\n",
        "Now we can put the actor and critic together using the A3C algorithm. It turns out, the tasks in the gym are all so simple that there is essentially no gain in parallelization, so technically we're implementing A2C (no async), but the RL part is the same."
      ]
    },
    {
      "metadata": {
        "id": "QoZgaABTksIL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class A3C(nn.Module):\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, discount=0.97, lr=1e-3, weight_decay=1e-4):\n",
        "        super(A3C, self).__init__()\n",
        "        self.actor = Actor(state_dim, action_dim)\n",
        "        self.critic = Critic(state_dim)\n",
        "        \n",
        "        # TODO: create an optimizer for the parameters of your actor (HINT: use the passed in lr and weight_decay args)\n",
        "        # (HINT: the actor and critic have different objectives, so how many optimizers do you need?)\n",
        "        \n",
        "        # do i use model.parameters() or self.actor.parameters() ?\n",
        "        self.actor_optim = optim.SGD(self.actor.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        self.critic_optim = optim.SGD(self.critic.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        \n",
        "        self.discount = discount\n",
        "        \n",
        "    def forward(self, state):\n",
        "        return self.actor.get_action(state)\n",
        "    \n",
        "    def learn(self, states, actions, rewards):\n",
        "        \n",
        "        returns = [compute_returns(rs, discount=self.discount) for rs in rewards]\n",
        "        \n",
        "        states, actions, returns = torch.cat(states), torch.cat(actions), torch.cat(returns)\n",
        "        \n",
        "        # TODO: implement A3C (HINT: algorithm details found in A3C paper supplement) \n",
        "        # (HINT2: the algorithm is actually very similar to REINFORCE, the only difference is now we have a critic, what might that do?)\n",
        "        \n",
        "        # start paste\n",
        "        error = F.mse_loss(self.critic.forward(states).squeeze(), returns).detach()\n",
        "        \n",
        "        Q_sa = returns\n",
        "        \n",
        "        V_st = self.critic.forward(states).squeeze()\n",
        "        \n",
        "        log = torch.log\n",
        "        \n",
        "        policy = self.actor.forward(states)\n",
        "        \n",
        "        pi_theta = torch.squeeze(policy.gather(1, actions.view(-1, 1)))\n",
        "\n",
        "        self.actor_optim.zero_grad()\n",
        "                \n",
        "        loss = -((Q_sa - V_st) * log(pi_theta))\n",
        "                \n",
        "        loss.sum().backward()\n",
        "                \n",
        "        self.actor_optim.step()\n",
        "        \n",
        "        self.critic_optim.zero_grad()\n",
        "        \n",
        "        self.critic_optim.step()\n",
        "        \n",
        "        return error.item() # Returns a rough estimate of the error in the baseline (dont worry about this too much)\n",
        "        \n",
        "        # end paste\n",
        "        \n",
        "        #raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hq-zQWzAksIO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1: Balancing a pole with a cart\n",
        "\n",
        "First, we'll test both algorithms on a very simple toy system: the cartpole. Eventhough it's very low dimensional (state=4, action=2), this task is nontrival because it is underactuated. Nevertheless after a few thousand episodes our policy shouldn't have a problem! "
      ]
    },
    {
      "metadata": {
        "id": "RGBdP7leksIP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Optimization hyperparameters\n",
        "lr = 1e-3\n",
        "weight_decay = 1e-4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8vAi8sXuksIS",
        "colab_type": "code",
        "outputId": "0034deb8-00cf-49c0-99c5-4cefb8455bc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "#env_name = 'CartPole-v1' \n",
        "#env_name = 'LunarLander-v2'\n",
        "# env_name = 'MountainCar-v0'\n",
        " env_name = 'Acrobot-v1'\n",
        "e = Pytorch_Gym_Env(env_name)\n",
        "state_dim = e.observation_space.shape[0]\n",
        "action_dim = e.action_space.n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "Meq1LT8NksIV",
        "colab_type": "code",
        "outputId": "8c4f2fb4-b8df-47e8-abbc-43666d1cb589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "cell_type": "code",
      "source": [
        "# Choose what agent to use\n",
        "#agent = REINFORCE(state_dim, action_dim, lr=lr, weight_decay=weight_decay)\n",
        "agent = A3C(state_dim, action_dim, lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "total_episodes = 0\n",
        "print(agent) # Let's take a look at what we're working with..."
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A3C(\n",
            "  (actor): Actor(\n",
            "    (fc1): Linear(in_features=6, out_features=4, bias=True)\n",
            "    (fc2): Linear(in_features=4, out_features=3, bias=True)\n",
            "    (fc9): Linear(in_features=6, out_features=3, bias=True)\n",
            "    (softmax): Softmax()\n",
            "  )\n",
            "  (critic): Critic(\n",
            "    (fc1): Linear(in_features=6, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WUl_VOypksIX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a \n",
        "gen = Generator(e, agent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ja001im9ksIZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Let's do this!!\n",
        "\n",
        "Below is the loop to train and evaluate your agent. You can play around with the number of iterations to run, and the number of rollouts per iteration. \n",
        "\n",
        "You can rerun this cell multiple times to keep training your model for more episodes. In any case, it shouldn't take more than 30 min to an 1 hour to train. (training never took me more than 5 min). HINT: Keep an eye on the eval_reward, it'll be pretty noisy, but if that should be slowly increasing."
      ]
    },
    {
      "metadata": {
        "id": "8oT0vzayksIa",
        "colab_type": "code",
        "outputId": "ac57c612-526e-403a-d8bb-0362db498082",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1201
        }
      },
      "cell_type": "code",
      "source": [
        "num_iter = 100\n",
        "num_train = 10\n",
        "num_eval = 10 # dont change this\n",
        "for itr in range(num_iter):\n",
        "    #agent.model.epsilon = epsilon * epsilon_decay ** (total_episodes / epsilon_decay_episodes)\n",
        "    #print('** Iteration {}/{} **'.format(itr+1, num_iter))\n",
        "    train_reward, train_loss = run_iteration('train', num_train, agent, gen)\n",
        "    eval_reward, _ = run_iteration('eval', num_eval, agent, gen)\n",
        "    total_episodes += num_train\n",
        "    print('Ep:{}: reward={:.3f}, loss={:.3f}, eval={:.3f}'.format(total_episodes, train_reward, train_loss, eval_reward))\n",
        "    \n",
        "    if eval_reward > 499 and env_name == 'CartPole-v1': # dont change this\n",
        "        print('Success!!! You have solved cartpole task! Time for a bigger challenge!')\n",
        "    \n",
        "    # save model\n",
        "print('Done')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ep:10: reward=-180.100, loss=834.788, eval=-500.000\n",
            "Ep:20: reward=-500.000, loss=1033.872, eval=-500.000\n",
            "Ep:30: reward=-500.000, loss=1033.934, eval=-500.000\n",
            "Ep:40: reward=-500.000, loss=1033.416, eval=-500.000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-a13422f7c5d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#agent.model.epsilon = epsilon * epsilon_decay ** (total_episodes / epsilon_decay_episodes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#print('** Iteration {}/{} **'.format(itr+1, num_iter))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0meval_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtotal_episodes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-758d89aba045>\u001b[0m in \u001b[0;36mrun_iteration\u001b[0;34m(mode, N, agent, gen, horizon, render)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-758d89aba045>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/rlhw_util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, horizon, render)\u001b[0m\n\u001b[1;32m     80\u001b[0m                         \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                         \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/rlhw_util.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/acrobot.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0ms_augmented\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorque\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrk4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dsdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_augmented\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;31m# only care about final timestep of integration returned by integrator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/acrobot.py\u001b[0m in \u001b[0;36mrk4\u001b[0;34m(derivs, y0, t, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mk3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mderivs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdt2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthist\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mk4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mderivs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthist\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0myout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m6.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk3\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0myout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "sxlxf0YwksIc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# You can visualize your policy at any time\n",
        "run_iteration('eval', 1, agent, gen, render=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "0SrE9Kh0ksIe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Analysis\n",
        "\n",
        "Plot the performance of each of your agents for the cartpole task and one additional task. When choosing a new environment, make sure is has a discrete action space. For each plot the x axis should show the total number of episodes the model was trained on, and the y axis shows the average total reward per episode.\n",
        "\n",
        "You can leave the plots as cell outputs below, or you can save them as images and submit them separately.\n",
        "\n",
        "### Deliverables\n",
        "- single plot showing both the REINFORCE algorithm's performance, and A3C's performance on the same plot for the cartpole environment (CartPole-v1).\n",
        "- single plot showing both the REINFORCE algorithm's performance, and A3C's performance on the same plot for a second environment of your choice (suggested -> LunarLander-v2, it's a little tricky but watching the agent fly spaceships is very entertaining!).\n",
        "- in every case you models have to learn something for full credit."
      ]
    },
    {
      "metadata": {
        "id": "90yijcV0ksIf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g0htUJVL7dfT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Plots were done in excel, they will be attached here as well as submitted speperatly\n",
        "\n",
        "![Cartpole](https://i.imgur.com/xMYCoXg.png)\n",
        "\n",
        "![Acrobot](https://i.imgur.com/4FJrTds.png)"
      ]
    }
  ]
}